{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sotof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.telemetry import tutorial_running\n",
    "from haystack.nodes import TextConverter, PDFToTextConverter, DocxToTextConverter, PreProcessor\n",
    "import logging\n",
    "from haystack.nodes import PreProcessor\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logs\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_docs = convert_files_to_docs(dir_path=doc_dir)\n",
    "\n",
    "converter = TextConverter(remove_numeric_tables=True, valid_languages=[\"en\"])\n",
    "doc_txt = converter.convert(file_path=\"prueba.txt\", meta=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import convert_files_to_docs\n",
    "\n",
    "\n",
    "all_docs = convert_files_to_docs(dir_path='tesis-qa-system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sotof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 66.56docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_docs_input: 1\n",
      "n_docs_output: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=100,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "\n",
    "docs_default = preprocessor.process([doc_txt])\n",
    "print(f\"n_docs_input: 1\\nn_docs_output: {len(docs_default)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 663.13docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPECTING SENTENCE BOUNDARY\n",
      "End of document: \"..., is included as an eligible practice under EQIP. \"\n",
      "\n",
      "NOT RESPECTING SENTENCE BOUNDARY\n",
      "End of document: \"...EQIP funding is reserved for livestock operations.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Not respecting sentence boundary vs respecting sentence boundary\n",
    "\n",
    "preprocessor_nrsb = PreProcessor(split_respect_sentence_boundary=False)\n",
    "docs_nrsb = preprocessor_nrsb.process([doc_txt])\n",
    "\n",
    "print(\"RESPECTING SENTENCE BOUNDARY\")\n",
    "end_text = docs_default[0].content[-50:]\n",
    "print('End of document: \"...' + end_text + '\"')\n",
    "print()\n",
    "print(\"NOT RESPECTING SENTENCE BOUNDARY\")\n",
    "end_text_nrsb = docs_nrsb[0].content[-50:]\n",
    "print('End of document: \"...' + end_text_nrsb + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 500.39docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: \"The EQIP covers up to 75 percent of costs incurred...\"\n",
      "Document 2: \"of costs incurred and 100 percent of income foregone for...\"\n",
      "Document 3: \"income foregone for implementing eligible conservation practices for awardees. There...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sliding window approach\n",
    "\n",
    "preprocessor_sliding_window = PreProcessor(split_overlap=3, split_length=10, split_respect_sentence_boundary=False)\n",
    "docs_sliding_window = preprocessor_sliding_window.process([doc_txt])\n",
    "\n",
    "doc1 = docs_sliding_window[0].content[:200]\n",
    "doc2 = docs_sliding_window[1].content[:100]\n",
    "doc3 = docs_sliding_window[2].content[:100]\n",
    "\n",
    "print('Document 1: \"' + doc1 + '...\"')\n",
    "print('Document 2: \"' + doc2 + '...\"')\n",
    "print('Document 3: \"' + doc3 + '...\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 0docs [00:00, ?docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_files_input: 0\n",
      "n_docs_output: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_docs = convert_files_to_docs(dir_path='tesis-qa-system')\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=100,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "docs = preprocessor.process(all_docs)\n",
    "\n",
    "print(f\"n_files_input: {len(all_docs)}\\nn_docs_output: {len(docs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
